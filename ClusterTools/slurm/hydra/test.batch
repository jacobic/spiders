# @ shell=/bin/bash
# zsh is used as paths to conda and Astromatic software are in ~/.zshenv
#
# Sample script for LoadLeveler
#

# @ error = job.err.$(jobid)
# @ output = job.out.$(jobid)
# @ job_type = MPICH
# @ node = 2
# @ tasks_per_node = 16
## @ total_tasks = 8
# @ resources = ConsumableCpus(1)
# @ network.MPI = sn_all,not_shared,us
# @ wall_clock_limit = 00:02:00
# @ notification = complete
# @ notify_user = $(user)@rzg.mpg.de
# @ queue

module unload mpi.ibm
module load mpi.intel

# To be able to load virtual env (i.e. dependencies listed in enviroment.yml)
export PATH="$PATH:$HOME/.local/anaconda3/bin/"

# Activate virtual enviroment
source activate ps1

# Enviroment variables for Astromatic software.
export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$MKLROOT/lib/intel64_lin"
export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$HOME/.local/anaconda3/envs/ps1/lib"
export PATH="$PATH:$HOME/.local/sextractor-2.19.5/src"
export PATH="$PATH:$HOME/.local/psfex-3.17.1/src"
export PATH="$PATH:$HOME/.local/psfex-3.17.1/src/fits"
export PATH="$PATH:$HOME/.local/stiff-2.4.0/src/"
export PATH="$PATH:$HOME/.local/stiff-2.4.0/src/fits/"
export PATH="$PATH:$HOME/.local/swarp-2.38.0/src/"
export PATH="$PATH:$HOME/.local/swarp-2.38.0/src/fits/"

# Append PYTHONPATH in order to find all modules in the project directory.
export PYTHONPATH="${PYTHONPATH}:$HOME/PanPipes"
export PYTHONUNBUFFERED=1
export PYTHONSEED=0

# Ensure you have executable permissions on the program.
chmod u+x /u/${USER}/PanPipes/src/data/remote/test.py

echo "Running script"

# Run the program with IBM's Parallel Operating Enviroment.
mpiexec -n 2 --bind-to none /u/${USER}/PanPipes/src/data/remote/test.py

# Bidn to none overcomes teh default behaviour of binding to core, which prevents use of multiprocessing within MPI
# i.e. this option enables processes to be forked from the original processes without being bound to the same core as that process.

#TODO: This may still be improved with click!
# Run as an executable without invoking python directly (for now):
# Add a shebang: ```#!/usr/bin/env python```
# Note that the python that is used by an activated conda env
# is ${CONDA_PREFIX}/bin/python and not /usr/bin/python.
# Add execute permissions: ```chmod u+x path/to/script.py```

#The most important Loadleveler commands are:

#llsubmit job_script_name   (Submit a job script for execution.)
#llq    (Check the status of your job(s))
#llcancel job_id    (Cancel a job)
#llclass    (List the available batch classes)
#llcancel $(llq | grep jacobic | awk '{print $1}')     (kill all jobs)

# The HYDRA file systems /u and /ptmp are also mounted on the DRACO login nodes as /hydra/u and /hydra/ptmp. So you can easily copy files from HYDRA to DRACO.
#To see relative (adjusted) prio llq -l -c n0008 | grep '@\|q_sysprio'